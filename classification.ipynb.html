<html>
<head>
<title>classification.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
classification.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
</span><span class="s1"># The dataset 
 
The dataset I'm using for this classification task is: The Titanic Disaster (https://www.kaggle.com/competitions/titanic/data). The goal of this dataset is to predict if a person is going to survive the titanic or not based on these variables: PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin and Embarked. 
In this notebook I try to conclude with which variables have a strong correlation with the classifier Survived and if they should be used in a model 
 
# The Final Dataset 
## Data cleaning 
 
### Null-values 
Some columns contained a lot of null-values, especially Cabin and Age. I chose to remove the column Cabin due to it having over 600 out of 891 null values. 
For Age I created three functions: 
- one that uses the median age as the value for all null-values. 
- another that uses the mean age 
- the last one removes every row which has at least one column containing a null value 
 
For Embarked 
- For the two missing Embarked values I chose the most common Embarked location which was &quot;S&quot; 
 
 
### Normalizing 
I chose to crate bins containing age intervals and then one-hot encoded these intervals. 
I could've also normalized the column Fare, by since Fare and Pclass is basically the same thing I ended up with dropping this column. 
 
 
### Encoding 
I changed the datatypes of Pclass, AgeGroup and Embarked to category, so I could then one-hot encode them. 
 
 
### Feature selection 
I also chose to remove some columns. The reason for this is that I know that these columns will not affect the chances of survival. The columns I removed was: PassengerId, Name, Cabin and Ticket. 
- Removing PassengerId due to it holding no meaningful data for visualizing or ML 
- Removing name for the same reason 
- Ticket consists of the ticket number which probably have no correlation with survival 
- Cabin has way to many null-values to give us any meaningful info and therefor I removed it 
- I also ended up with removing Fare in The Final Dataset, due to it adding little extra value due to the Pclass column. 
 
## EDA 
### Feature engineering 
I chose to create a new variable FamilyMembersCount. This was created by adding SibSp and Parch together. This variable gives us and idea of how many people traveled together. A family size of one indicates that the person traveled alone. 
 
### Feature Correlation Analysis 
I see a great correlation between the ground truth and: Sex and Pclass, but a little unclear how Age correlates with Survived. The heatmap shows close to no correlation between Age and Survived, but independent analysis shows a great correlation. 
</span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>

<span class="s2">import </span><span class="s1">seaborn </span><span class="s2">as </span><span class="s1">sns</span>
<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>
<span class="s0">#%% 
# Load dataset</span>
<span class="s1">df = pd.read_csv(</span><span class="s3">'titanic.csv'</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">## Data preprocessing 
</span><span class="s0">#%% 
</span><span class="s1">df.shape</span>
<span class="s0">#%% 
</span><span class="s1">df.columns</span>
<span class="s0">#%% 
</span><span class="s1">df.info()</span>
<span class="s0">#%% 
</span><span class="s1">df.describe()</span>
<span class="s0">#%% md 
</span><span class="s1">Here you can see that the max fare price is over 15x the price of the 75%. This means that this a major outlier. I will therefor remove this value later on. 
</span><span class="s0">#%% 
</span><span class="s1">df.head()</span>
<span class="s0">#%% 
</span><span class="s1">df.tail()</span>
<span class="s0">#%% md 
</span><span class="s1"># Cleaning the dataset 
</span><span class="s0">#%% 
</span><span class="s1">df.isnull().sum()</span>
<span class="s0">#%% md 
</span><span class="s1">## Feature selection 
</span><span class="s0">#%% md 
</span><span class="s1">- Removing PassengerId due to it holding no meaningful data for visualizing or ML 
- Removing name for the same reason 
- Ticket consists of the ticket number which probably have no correlation with survival 
- Cabin has way to many null-values to give us any meaningful info and therefor I removed it 
</span><span class="s0">#%% 
</span><span class="s1">df = df.drop([</span><span class="s3">'PassengerId'</span><span class="s2">, </span><span class="s3">'Name'</span><span class="s2">, </span><span class="s3">'Ticket'</span><span class="s2">, </span><span class="s3">'Cabin'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">## Null-values 
</span><span class="s0">#%% 
# To find which place most people embarked from to set this as the embarked for Null values</span>
<span class="s1">max = df.groupby(</span><span class="s3">'Embarked'</span><span class="s1">).count()</span>
<span class="s0"># print(max)</span>

<span class="s0"># Sets null values equal to the median of that column</span>
<span class="s2">def </span><span class="s1">handle_null_median(df):</span>
    <span class="s0"># Need to set inplace=True, so it doesn't create a copy of the dataframe. Tried without and this led to null-values not being removed</span>
    <span class="s1">df[</span><span class="s3">'Fare'</span><span class="s1">].fillna(df[</span><span class="s3">'Fare'</span><span class="s1">].median()</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">df[</span><span class="s3">'Age'</span><span class="s1">].fillna(df[</span><span class="s3">'Age'</span><span class="s1">].median()</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">df[</span><span class="s3">'Embarked'</span><span class="s1">].fillna(</span><span class="s3">'S'</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s2">return  </span><span class="s1">df</span>

<span class="s0"># Sets null values equal to the mean of that column</span>
<span class="s2">def </span><span class="s1">handle_null_mean(df):</span>
    <span class="s1">df[</span><span class="s3">'Fare'</span><span class="s1">].fillna(df[</span><span class="s3">'Fare'</span><span class="s1">].mean()</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">df[</span><span class="s3">'Age'</span><span class="s1">].fillna(df[</span><span class="s3">'Age'</span><span class="s1">].mean()</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s1">df[</span><span class="s3">'Embarked'</span><span class="s1">].fillna(</span><span class="s3">'S'</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s2">return  </span><span class="s1">df</span>

<span class="s0"># Function to drop all rows with at least 1 null value. (Removes way to many rows, so isn't a good option)</span>
<span class="s2">def </span><span class="s1">null_values_drop(df):</span>
    <span class="s1">df_copy = df.copy()</span>
    <span class="s1">df_drop = df_copy.dropna(axis=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">how=</span><span class="s3">'any'</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">df_drop</span>

<span class="s1">df = handle_null_median(df)</span>
<span class="s1">df</span>
<span class="s0">#%% 
# Check the new sum of nulls</span>
<span class="s1">df.isnull().sum()</span>
<span class="s0">#%% 
# Encoding Sex to be represented by either ones or zeroes</span>
<span class="s1">df[</span><span class="s3">'Sex'</span><span class="s1">] = df[</span><span class="s3">'Sex'</span><span class="s1">].replace([</span><span class="s3">'female'</span><span class="s2">, </span><span class="s3">'male'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">,</span><span class="s4">1</span><span class="s1">])</span>
<span class="s1">df</span>
<span class="s0">#%% md 
</span><span class="s1">## Normalizing 
</span><span class="s0">#%% 
</span><span class="s1">pd.cut(df[</span><span class="s3">'Age'</span><span class="s1">]</span><span class="s2">, </span><span class="s4">5</span><span class="s1">)</span>
<span class="s0">#%% 
# Got the idea of splitting age into bins from: https://www.kaggle.com/code/sinakhorami/titanic-best-working-classifier, but seems like a common practice called Binning, where you put a range of values into a bin</span>
<span class="s2">def </span><span class="s1">bins(df): </span><span class="s0"># This is kind of feature engineering as well</span>
    <span class="s1">df[</span><span class="s3">'AgeGroup'</span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s1">df.loc[df[</span><span class="s3">'Age'</span><span class="s1">] &lt; </span><span class="s4">16.336</span><span class="s2">, </span><span class="s3">'AgeGroup' </span><span class="s1">] = </span><span class="s4">0</span>
    <span class="s1">df.loc[(df[</span><span class="s3">'Age'</span><span class="s1">] &gt;= </span><span class="s4">16.336</span><span class="s1">) &amp; (df[</span><span class="s3">'Age'</span><span class="s1">] &lt; </span><span class="s4">32.252</span><span class="s1">)</span><span class="s2">, </span><span class="s3">'AgeGroup'</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">df.loc[(df[</span><span class="s3">'Age'</span><span class="s1">] &gt;= </span><span class="s4">32.252</span><span class="s1">) &amp; (df[</span><span class="s3">'Age'</span><span class="s1">] &lt; </span><span class="s4">48.168</span><span class="s1">)</span><span class="s2">, </span><span class="s3">'AgeGroup'</span><span class="s1">] = </span><span class="s4">2</span>
    <span class="s1">df.loc[(df[</span><span class="s3">'Age'</span><span class="s1">] &gt;= </span><span class="s4">48.168</span><span class="s1">) &amp; (df[</span><span class="s3">'Age'</span><span class="s1">] &lt; </span><span class="s4">64.084</span><span class="s1">)</span><span class="s2">, </span><span class="s3">'AgeGroup'</span><span class="s1">] = </span><span class="s4">3</span>
    <span class="s1">df.loc[df[</span><span class="s3">'Age'</span><span class="s1">] &gt;= </span><span class="s4">64.084</span><span class="s2">, </span><span class="s3">'AgeGroup'</span><span class="s1">] = </span><span class="s4">4</span>

    <span class="s0"># Could also create bins for fare, but not sure Fare is needed.</span>

    <span class="s2">return </span><span class="s1">df</span>

<span class="s2">def </span><span class="s1">min_max_normalizing(df):</span>
    <span class="s1">df[</span><span class="s3">'Age'</span><span class="s1">] = (df[</span><span class="s3">'Age'</span><span class="s1">] - df[</span><span class="s3">'Age'</span><span class="s1">].min()) / \</span>
                      <span class="s1">(df[</span><span class="s3">'Age'</span><span class="s1">].max() - df[</span><span class="s3">'Age'</span><span class="s1">].min())</span>
    <span class="s1">df[</span><span class="s3">'Fare'</span><span class="s1">] = (df[</span><span class="s3">'Fare'</span><span class="s1">] - df[</span><span class="s3">'Fare'</span><span class="s1">].min()) / \</span>
                      <span class="s1">(df[</span><span class="s3">'Fare'</span><span class="s1">].max() - df[</span><span class="s3">'Fare'</span><span class="s1">].min())</span>

    <span class="s2">return </span><span class="s1">df</span>

<span class="s0"># Z-score Normalizing</span>
<span class="s2">def </span><span class="s1">z_score_normalizing(df):</span>
    <span class="s1">df[</span><span class="s3">'Age'</span><span class="s1">] = (df[</span><span class="s3">'Age'</span><span class="s1">] - df[</span><span class="s3">'Age'</span><span class="s1">].mean()) / df[</span><span class="s3">'Age'</span><span class="s1">].std()</span>
    <span class="s1">df[</span><span class="s3">'Fare'</span><span class="s1">] = (df[</span><span class="s3">'Fare'</span><span class="s1">] - df[</span><span class="s3">'Fare'</span><span class="s1">].mean()) / df[</span><span class="s3">'Fare'</span><span class="s1">].std()</span>

    <span class="s2">return </span><span class="s1">df</span>

<span class="s1">df = bins(df)</span>
<span class="s1">df</span>
<span class="s0">#%% md 
</span><span class="s1"># EDA 
</span><span class="s0">#%% md 
</span><span class="s1">## Feature Engineering 
</span><span class="s0">#%% 
# Got this idea from a Titanic notebook: https://www.kaggle.com/code/sinakhorami/titanic-best-working-classifier</span>
<span class="s1">df[</span><span class="s3">'FamilyMembersCount'</span><span class="s1">] = df[</span><span class="s3">'Parch'</span><span class="s1">] + df[</span><span class="s3">'SibSp'</span><span class="s1">] + </span><span class="s4">1</span>
<span class="s1">df[[</span><span class="s3">'Parch'</span><span class="s2">, </span><span class="s3">'SibSp'</span><span class="s2">, </span><span class="s3">'FamilyMembersCount'</span><span class="s1">]]</span>
<span class="s0">#%% 
# Drop columns no longer used after feature engineering</span>
<span class="s1">df = df.drop([</span><span class="s3">'Parch'</span><span class="s2">, </span><span class="s3">'SibSp'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">df</span>
<span class="s0">#%% md 
</span><span class="s1">## Feature Correlation Investigation 
</span><span class="s0">#%% 
</span><span class="s1">df[[</span><span class="s3">'Sex'</span><span class="s2">, </span><span class="s3">'Survived'</span><span class="s1">]].groupby([</span><span class="s3">'Sex'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">as_index=</span><span class="s2">False</span><span class="s1">).mean()</span>
<span class="s0">#%% md 
</span><span class="s1">The biggest indicator if you were going to survive the Titanic was your gender. Back in 1912 the feminist movement had not come as far as they have today and therefor the saying &quot;Women and children first&quot; was taken a little to literal. From this statistic we can conclude: 
- Men were less prioritized and only 19% of all men onboard survived, while 74% of all females survived 
</span><span class="s0">#%% 
</span><span class="s1">df[[</span><span class="s3">'Pclass'</span><span class="s2">, </span><span class="s3">'Survived'</span><span class="s1">]].groupby([</span><span class="s3">'Pclass'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">as_index=</span><span class="s2">False</span><span class="s1">).mean()</span>
<span class="s0">#%% md 
</span><span class="s1">Here we can see a clear correlation between survival and Ticket class. The higher your class the more likely you were of surviving. This correlation can come from a wide range of different reasons. The two reasons I find the most likely is: 
- Higher classes were situated closer to the main deck and therefore had easier access to the few lifeboats. On the contrary, the lower class(Pclass 3) was most often situated at the lower decks and therefore had a greater chance of dying from the initial floding of the boat 
- Higher classes were also most likely prioritized when it came to which was getting a lifeboat 
</span><span class="s0">#%% 
</span><span class="s1">df[[</span><span class="s3">'FamilyMembersCount'</span><span class="s2">, </span><span class="s3">'Survived'</span><span class="s1">]].groupby([</span><span class="s3">'FamilyMembersCount'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">as_index=</span><span class="s2">False</span><span class="s1">).mean().sort_values(by=</span><span class="s3">'Survived'</span><span class="s2">, </span><span class="s1">ascending=</span><span class="s2">False</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">Here we can see that people traveling with a FamilySize of 4 had the greatest chance of survival: 
- This could be a result of higher classes normally having smaller FamilySizes. 
 
We can also see that with a FamilySize over 7, your chance of survival was 0%. This could be because of a handful of reasons, but most likely: 
- A low number of families were larger than 7 and by random they all died. Let's say it was only two families bigger than 7 and just by chance they all died 
- Bigger FamilySize = Lower class = Lower chance of survival from earlier analysis 
- Could also be due to collective suicide. Their mentality could've been since all of us most likely won't survive, let's end it together. 
 
With a FamilySize of 1, the person was traveling alone. When traveling alone your chances of surviving was amongst the smallest. I accredit this to one main reason: 
- People traveling alone was most likely males and therefor from previous analysis had the lowest chance of survival. 
</span><span class="s0">#%% 
</span><span class="s1">df[[</span><span class="s3">'FamilyMembersCount'</span><span class="s2">, </span><span class="s3">'Sex'</span><span class="s1">]].groupby([</span><span class="s3">'FamilyMembersCount'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">as_index=</span><span class="s2">False</span><span class="s1">).mean().sort_values(by=</span><span class="s3">'FamilyMembersCount'</span><span class="s2">, </span><span class="s1">ascending=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">This statistic is to back up my last claim of people traveling alone was most likely males, and therefor more likely to die. Here we can see that: 
- 76% of all traveling alone, were male 
</span><span class="s0">#%% 
</span><span class="s1">df[[</span><span class="s3">'Embarked'</span><span class="s2">, </span><span class="s3">'Survived'</span><span class="s1">]].groupby([</span><span class="s3">'Embarked'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">as_index=</span><span class="s2">False</span><span class="s1">).mean()</span>
<span class="s0">#%% md 
</span><span class="s1">With a quick google search we know that C = Cherbourg, Q = Queenstown and S = Southampton. Q and S have basically the same survival rate, but C outperforms these with 20% percent. I think the main reason for this is: 
- Cherbourg being a richer city than Queenstown and Southampton and therefor more people traveling in first class 
</span><span class="s0">#%% 
</span><span class="s1">df[[</span><span class="s3">'AgeGroup'</span><span class="s2">, </span><span class="s3">'Survived'</span><span class="s1">]].groupby([</span><span class="s3">'AgeGroup'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">as_index=</span><span class="s2">False</span><span class="s1">).mean()</span>
<span class="s0">#%% md 
</span><span class="s1">Here we can see that if you belonged to AgeGroup 0 (16 or younger) you had the greatest chance of survival. 
</span><span class="s0">#%% 
</span><span class="s1">df.hist(column=</span><span class="s3">'Age'</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">Most people are between mid 20's and early 30's. With some outliers closing in on 80. If you take a look at the next plot; most pople dying is in this exact Age range. 
</span><span class="s0">#%% 
</span><span class="s1">survived_by_age = df.plot(column=</span><span class="s3">'Age'</span><span class="s2">,</span>
        <span class="s1">by=</span><span class="s3">'Survived'</span><span class="s2">,</span>
        <span class="s1">kind=</span><span class="s3">'hist'</span><span class="s2">,</span>
        <span class="s1">alpha=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">rot=</span><span class="s4">45</span><span class="s2">,</span>
        <span class="s1">grid=</span><span class="s2">True,</span>
        <span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">This visual representation gives us an idea of how many people died and survived for each age group. From this representation we can conclude: 
- Most kids under the age of 10 survived. (Women and children first mentality) 
- Most death amongst people in the age of 20-35. (Most people on the boat was in this age group) 
- No-one over the age of late 70's survived. (Probably de-prioritized) 
</span><span class="s0">#%% 
</span><span class="s1">df[</span><span class="s3">'Pclass'</span><span class="s1">] =  df[</span><span class="s3">'Pclass'</span><span class="s1">].astype(</span><span class="s3">'category'</span><span class="s1">)</span>
<span class="s1">df[</span><span class="s3">'Embarked'</span><span class="s1">] =  df[</span><span class="s3">'Embarked'</span><span class="s1">].astype(</span><span class="s3">'category'</span><span class="s1">)</span>
<span class="s1">df[</span><span class="s3">'AgeGroup'</span><span class="s1">] =  df[</span><span class="s3">'AgeGroup'</span><span class="s1">].astype(</span><span class="s3">'category'</span><span class="s1">)</span>
<span class="s1">df.info()</span>
<span class="s0">#%% 
</span><span class="s1">df = pd.get_dummies(df)</span>
<span class="s0">#%% 
</span><span class="s1">corr = df.corr()</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">32</span><span class="s2">, </span><span class="s4">12</span><span class="s1">))</span>
<span class="s1">heatmap = sns.heatmap(corr</span><span class="s2">, </span><span class="s1">annot=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">Heatmaps are a great way of visualizing how one attribute affects another. The closer a value gets to 1 or -1 the stronger the correlation is. 0 means no correlation 
There's two types of correlation: 
- Positive correlation: when one variable increases another increases. Ex.: Fare and Survived 
- Negative correlation: when one variable increases/decreases another does the opposite. Ex.: Sex_1 and Survived 
 
This heatmap is a clear indicator that Gender plays a major role in someone's chances of survival, the same does Pclass and Embarked. 
</span><span class="s0">#%% md 
</span><span class="s1"># The Final Dataset 
</span><span class="s0">#%% md 
</span><span class="s1">I'm dropping Age due to the Agegroup column. 
I'm dropping Fare due to it not being an independent variable since it's strongly correlated with Pclass. 
</span><span class="s0">#%% 
</span><span class="s1">df = df.drop(columns=[</span><span class="s3">'Age'</span><span class="s2">, </span><span class="s3">'Fare'</span><span class="s1">])</span>
<span class="s1">df</span>
<span class="s0">#%% md 
</span><span class="s1"># Splitting the Dataset 
</span><span class="s0">#%% 
</span><span class="s1">train = df.sample(frac=</span><span class="s4">0.6</span><span class="s1">)</span>
<span class="s1">val_test = df.drop(train.index)</span>
<span class="s1">val = val_test.sample(frac=</span><span class="s4">0.5</span><span class="s1">)</span>
<span class="s1">test = val_test.drop(val.index)</span></pre>
</body>
</html>